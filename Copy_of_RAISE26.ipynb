{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seleenabusi07/raise26-ai-headlines-analysis/blob/main/Copy_of_RAISE26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GxkCLaxdtPRU",
        "outputId": "9238dd57-faeb-468c-d487-8bcdcd1852f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dataset_B_news_subset_3500.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2378898908.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/dataset_B_news_subset_3500.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset_B_news_subset_3500.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/dataset_B_news_subset_3500.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "pnNKuwpMvM0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"classes_str\"].value_counts().head(10)"
      ],
      "metadata": {
        "id": "7rVP6pEaCT9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most frequent category in AI-related headlines is Learning, Knowledge & Education, followed closely by Work, Jobs & Economy. This suggests that media coverage most often frames AI as a force shaping how people learn and how they work. Everyday life and lifestyle changes are also prominent, indicating that AI is increasingly portrayed as being integrated in daily human routines rather than only as a technical tool.\n"
      ],
      "metadata": {
        "id": "MQi6LwHUCwTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_classes = df[\"classes_str\"].value_counts().head(10)\n",
        "\n",
        "top_classes.plot(kind=\"bar\")\n",
        "plt.title(\"Top 10 Human Behavior Categories in AI News Headlines\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Number of Headlines\")\n",
        "plt.xticks(rotation=75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tS6Nbt2xDNIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "FOiGgigAhG2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out headlines not classed as education or work"
      ],
      "metadata": {
        "id": "ebqGiv4sh2Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_list = ['Learning, Knowledge & Education', 'Work, Jobs & Economy']\n",
        "df[df['classes_str'].isin(classes_list)]"
      ],
      "metadata": {
        "id": "InVk17H9hJ3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing required libraries for sentiment analysis"
      ],
      "metadata": {
        "id": "Eqo9Ge2NhzhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "5iHwvFgDhPel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA CLEANING"
      ],
      "metadata": {
        "id": "gt5ciw3XhxRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining data types\n",
        "data_types = df.dtypes\n",
        "\n",
        "print(data_types)"
      ],
      "metadata": {
        "id": "u8griNARhU8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for duplicated values"
      ],
      "metadata": {
        "id": "LtLdxOzxhvCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated()"
      ],
      "metadata": {
        "id": "X-60eXpzhV3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting cols into categorical and numerical"
      ],
      "metadata": {
        "id": "sFk6j_Ghhs3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_col = [col for col in df.columns if df[col].dtype == 'object']\n",
        "num_col = [col for col in df.columns if df[col].dtype != 'object']\n",
        "\n",
        "print('Categorical columns:', cat_col)\n",
        "print('Numerical columns:', num_col)"
      ],
      "metadata": {
        "id": "9mchnXCzhaP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for unique values"
      ],
      "metadata": {
        "id": "sa_NNNychqdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[cat_col].nunique()"
      ],
      "metadata": {
        "id": "51wuz570hdRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for null values"
      ],
      "metadata": {
        "id": "almaE9lohoS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull()"
      ],
      "metadata": {
        "id": "1Y9k5XJbhfiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qPWSpZ_mhgz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text normalization: converting text to lowercase, removing punctuation, and removing unecessary whitespace"
      ],
      "metadata": {
        "id": "rFJLWv_7lKtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['normalized_text'] = df['title'].str.lower()"
      ],
      "metadata": {
        "id": "ag6ihfLUlOnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "df['normalized_text'] = df['title'].str.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "jjqaeExTlgda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['normalized_text'] = df['normalized_text'].str.strip()"
      ],
      "metadata": {
        "id": "xHGXs-cjmXeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['normalized_text'] = df['normalized_text'].str.replace(r'\\s+', ' ', regex=True)"
      ],
      "metadata": {
        "id": "8QWyKw2hme6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "R3Ffa6armyPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove special characters and any extra whitespace"
      ],
      "metadata": {
        "id": "sDBczJpcy9Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_noise(text):\n",
        "    text = re.sub(r'[^È´\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['normalized_text'] = df['normalized_text'].apply(remove_noise)"
      ],
      "metadata": {
        "id": "fKHhyN-4m-HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mgGp2N-lyrtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZATION"
      ],
      "metadata": {
        "id": "swTSQtoXisku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing nltk libraries"
      ],
      "metadata": {
        "id": "R3QWaN6K1zMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "KN_hKDUNjtzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "cSHqVRqB1gSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "wMCuaken1msW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove stopwords"
      ],
      "metadata": {
        "id": "ARvXQ-H3Q-5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [word for word in word_tokens \\\n",
        "                     if word.lower() not in stop_words]\n",
        "  return ' '.join(filtered_text)"
      ],
      "metadata": {
        "id": "FmhfSTOa15x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['filtered_text'] = df['normalized_text'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "ToG-Oi8Q2Ngc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "UnqtKdzU2bBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Hxr7SFRs2kVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming and lemmatization"
      ],
      "metadata": {
        "id": "MqSS34nQ2wp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "iEc4eRzO2vn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "Z65f39rk2_X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "LxVeUEAG3Iy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for stemming and lemmatizing each word"
      ],
      "metadata": {
        "id": "ibf9Bn0jRINm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_stemming(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
        "  return ' '.join(stemmed_words)"
      ],
      "metadata": {
        "id": "UdHL07rK3POs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_lemmatization(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "  return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "N3K0zSI-3eq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['stemmed_text'] = df['filtered_text'].apply(perform_stemming)\n",
        "df['lemmatized_text'] = df['stemmed_text'].apply(perform_lemmatization)"
      ],
      "metadata": {
        "id": "LmYaT1nn3r2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eq_55z9a4B-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting text to numbers using tf-idf technique"
      ],
      "metadata": {
        "id": "hYGoIYuG4Hra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "qu-t1OWn4TUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "6XHtiv364nav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = count_vectorizer.fit_transform(df['lemmatized_text'])"
      ],
      "metadata": {
        "id": "dxpkRg2x4zDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "bY7z_KRG48rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow_df)"
      ],
      "metadata": {
        "id": "_QsP0tnp5JXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_text'])"
      ],
      "metadata": {
        "id": "YwS4prpw5PuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "m2amfhGH5YiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_df)"
      ],
      "metadata": {
        "id": "ECZeN4um5dNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "Np11U7t56hoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "ofaaK00U6weH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "DmwVHXQH63or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "EiT7Woxj6jtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to return binary value for sentiment"
      ],
      "metadata": {
        "id": "eMvrzHeZRT9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "  scores = analyzer.polarity_scores(text)\n",
        "  sentiment = 1 if scores['pos'] > 0 else 0\n",
        "  return sentiment"
      ],
      "metadata": {
        "id": "ciIgL4Jd6688"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['lemmatized_text'].apply(get_sentiment)"
      ],
      "metadata": {
        "id": "p73CiU037HGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "554CrBmv7Ns_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "TbjtOO617RnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disregarding binary sentiment col to instead class sentiment as positive, neutral, negative, or compound (more inclusive)"
      ],
      "metadata": {
        "id": "B7caiCUpRgAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('sentiment', axis=1)"
      ],
      "metadata": {
        "id": "EeXoRfQo7shO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyzing first row using new technique"
      ],
      "metadata": {
        "id": "YrXiWX3SRumg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer.polarity_scores(['lemmatized_text'][0])"
      ],
      "metadata": {
        "id": "tTw2QIji8PQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer.polarity_scores(['stemmed_text'][0])"
      ],
      "metadata": {
        "id": "77GMklcd8dLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to apply technique to the rest of the cells"
      ],
      "metadata": {
        "id": "-7GmFwkJR1bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body = df.lemmatized_text\n",
        "neg, neu, pos, compound = [], [], [], []\n",
        "for headline in body:\n",
        "  res = analyzer.polarity_scores(str(headline))\n",
        "  neg.append(res['neg'])\n",
        "  neu.append(res['neu'])\n",
        "  pos.append(res['pos'])\n",
        "  compound.append(res['compound'])"
      ],
      "metadata": {
        "id": "IzA-YLqn8ufe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Negative\"] = neg\n",
        "df[\"Neutral\"] = neu\n",
        "df[\"Positive\"] = pos\n",
        "df[\"Compound\"] = compound"
      ],
      "metadata": {
        "id": "MRt-n7rT9cuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "pG3ePGjC92Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning either positive or negative sentiment tag for classing headlines based on pos, neg metrics from earlier"
      ],
      "metadata": {
        "id": "H3AyTJIcR7-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag=[]\n",
        "for i in range(len(df.lemmatized_text)):\n",
        "  winning_val = max(neg[i], pos[i])\n",
        "  if (winning_val == neg[i]):\n",
        "    tag.append(\"Negative\")\n",
        "  elif(pos[i]==winning_val):\n",
        "    tag.append(\"Positive\")\n",
        "\n",
        "df[\"Sentiment_Tag\"]=tag"
      ],
      "metadata": {
        "id": "Y7hG02sM-BBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "JvScGZUv-5jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_list1 = ['Learning, Knowledge & Education', 'Work, Jobs & Economy']\n",
        "df = df[df['classes_str'].isin(classes_list1)]"
      ],
      "metadata": {
        "id": "38QHS5tBPY-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = df['Sentiment_Tag'][df['Sentiment_Tag']==\"Positive\"].count()\n",
        "nn = df['Sentiment_Tag'][df['Sentiment_Tag']==\"Negative\"].count()"
      ],
      "metadata": {
        "id": "-R4yJt9Y_lF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Positive Headlines:\", pp)\n",
        "print(\"Number of Negative Headlines:\", nn)"
      ],
      "metadata": {
        "id": "T9X2bbhpAaMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRELATION ANALYSIS"
      ],
      "metadata": {
        "id": "6_Sy03VREdHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "-Kadv-HjEezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation = df['sentiment'].corr(df['quarter'])"
      ],
      "metadata": {
        "id": "Q26p9keAVKWl",
        "outputId": "829177c3-0ccf-46b3-90fb-be88b1fdafcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-657676690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quarter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# negligible correlation, checking for percentage distribution of positive and negative headlines instead"
      ],
      "metadata": {
        "id": "G8a2WKXPYH8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Correlation: {correlation}\")"
      ],
      "metadata": {
        "id": "mkpKOZzQVeKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby('quarter')['sentiment'].value_counts(normalize=True).unstack()"
      ],
      "metadata": {
        "id": "in5Axf_Cbfba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grouped)"
      ],
      "metadata": {
        "id": "4vbhTw-GVqHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df.groupby(['quarter', 'sentiment']).size().reset_index(name='count')"
      ],
      "metadata": {
        "id": "8M5JiYkdV2mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_counts_per_quarter = counts.groupby('quarter')['count'].transform('sum')\n",
        "counts['percent'] = (counts['count'] / total_counts_per_quarter) * 100"
      ],
      "metadata": {
        "id": "BTyMquHrWcgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = counts.pivot(index='quarter', columns='sentiment', values='percent')\n",
        "print(table)"
      ],
      "metadata": {
        "id": "UWhctC8fXQf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table.plot(kind='bar', stacked=True)\n"
      ],
      "metadata": {
        "id": "FKoOAQe1XVz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all articles abt work and education were only written in q3..."
      ],
      "metadata": {
        "id": "20UeTDE0YTdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quarter_list = [1, 2, 4]\n",
        "df[df['quarter'].isin(quarter_list)]"
      ],
      "metadata": {
        "id": "G-OToS_AXvye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "After initial plot of the data, it became apparent that the majority of the headlines (665/3500) were classed under Work or Education, so those headlines were the focus of this project to see how AI impacts sentiment in professional spaces like work and school. The dataset was preprocessed, first checking for any null, duplicate or missing values (none found, indicating clean data). The headlines were then normalized by converting text to lowercase, removing punctuation, special characters, and unnecessary whitespace. They were then tokenized by word, removing stopwords before stemming and lemmatization to reduce words to their most basic forms. Using TF-IDF, the tokens were converted into numerical data for sentiment analysis, and then assigned values from 0 to 1 in four categories; positive, negative, neutral, and compound. Headlines were then assigned a sentiment tag of either \"Positive\" or \"Negative\" depending on these values and tallied. There were a total of 135 \"Positive\" headlines and 520 \"Negative\" headlines, nearly 3.85x as many negative headlines in comparison to positive. A correlation analysis was then performed to see if there's any correlation between the sentiment tag and the quarter the article was published, resulting in a negligible result as all articles in these two classes were written in Q3."
      ],
      "metadata": {
        "id": "b3KvngpGYu6z"
      }
    }
  ]
}